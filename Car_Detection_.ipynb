{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivorima/YOLOv5-CarDetection/blob/main/Car_Detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1HbqIK212ak"
      },
      "source": [
        "## **Pre-requisites:**\n",
        "\n",
        "1.   having a google account (sorry about that...)\n",
        "2.   having the dataset.zip in drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXU6yC1grLBs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# initially I had a zip file but now I use the folder directly, change the paths accordingly\n",
        "# !unzip \"path/to/dataset.zip\" -d \"path/to/extract\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn-lrIKfPgMk"
      },
      "source": [
        "# Creating the dataset class\n",
        "\n",
        "First Attempt: *we need to create a class derived from pytorch's Dataset class able to understand our data format to later on create a dataloader and feed it to our model*\n",
        "\n",
        "After choosing Yolo, I realized we don't really need a class, as Yolo takes direct paths to the data, so the class is not needed for this but could be uselful for other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo_m7sSR15WV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "# this is for the progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "folder = \"/content/drive/MyDrive/Bureau/VMI/Cars/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-0Af7FI8_t"
      },
      "outputs": [],
      "source": [
        "class CarDataset(Dataset):\n",
        "  def __init__(self, root_dir,transform=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "\n",
        "        # Load image filenames and annotations\n",
        "        for filename in tqdm(os.listdir(os.path.join(root_dir, 'images/'))):\n",
        "            if filename.endswith(\".png\"):\n",
        "                image_filename = os.path.join(os.path.join(root_dir, 'images/'), filename)\n",
        "                annotation_filename = os.path.join(os.path.join(root_dir, 'labels/'), filename).replace(\".png\", \".txt\")\n",
        "\n",
        "                # Load the image\n",
        "                image = cv2.imread(image_filename)\n",
        "\n",
        "                # Apply the transformations\n",
        "                if self.transform is not None:\n",
        "                   image = self.transform(image)\n",
        "\n",
        "                # populating my transformed data\n",
        "                self.images.append(image)\n",
        "\n",
        "                # Load the annotations\n",
        "                with open(annotation_filename, 'r') as f:\n",
        "                    annos = []\n",
        "                    for line in f:\n",
        "                        # Extract the bounding box coordinates\n",
        "                        class_label, x_min, y_min, x_max, y_max = line.strip().split(' ')\n",
        "                        class_label, x_min, y_min, x_max, y_max = str(class_label),float(x_min), float(y_min), float(x_max), float(y_max)\n",
        "\n",
        "                        # Just checking, in case there are some infiltrations\n",
        "                        if class_label == \"Car\":\n",
        "\n",
        "                          # Convert to YOLO format (x_center, y_center, width, height)\n",
        "                          x_center = (x_min + x_max) / 2.0\n",
        "                          y_center = (y_min + y_max) / 2.0\n",
        "                          width = x_max - x_min\n",
        "                          height = y_max - y_min\n",
        "\n",
        "                          # Normalize coordinates\n",
        "                          x_center /= image.shape[2]\n",
        "                          y_center /= image.shape[1]\n",
        "                          width /= image.shape[2]\n",
        "                          height /= image.shape[1]\n",
        "\n",
        "                          # Ensure values are within the valid range [0, 1]\n",
        "                          x_center = max(0.0, min(1.0, x_center))\n",
        "                          y_center = max(0.0, min(1.0, y_center))\n",
        "                          width = max(0.0, min(1.0, width))\n",
        "                          height = max(0.0, min(1.0, height))\n",
        "\n",
        "                          # since we only have one class, 'Car' we put the idx 0\n",
        "                          anno = (0, x_center, y_center, width, height)\n",
        "                          annos.append(anno)\n",
        "\n",
        "                    self.annotations.append(annos)\n",
        "\n",
        "        # split into train, val, test randomly\n",
        "        # im going to split 70-20-10 for this\n",
        "        self.train_images, rest_images, self.train_annotations, rest_annotations = train_test_split(self.images, self.annotations, test_size = 0.3, random_state = 1)\n",
        "        self.val_images, self.test_images, self.val_annotations, self.test_annotations = train_test_split(rest_images, rest_annotations, test_size = 0.3, random_state = 1)\n",
        "\n",
        "\n",
        "        # Save datasets into YOLOv5 format\n",
        "        self.save_dataset_yolov5('train', self.train_images, self.train_annotations)\n",
        "        self.save_dataset_yolov5('val', self.val_images, self.val_annotations)\n",
        "        self.save_dataset_yolov5('test', self.test_images, self.test_annotations)\n",
        "\n",
        "\n",
        "  # basic function that saves the data into the correct folder\n",
        "  def save_dataset_yolov5(self, split, images, annotations):\n",
        "      save_dir = os.path.join(self.root_dir, f'transformed/{split}/')\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "      for idx, (image, annotations_for_image) in enumerate(zip(images, annotations)):\n",
        "          # Save image\n",
        "          image_path = os.path.join(save_dir, f'{idx}.png')\n",
        "          # Convert PyTorch tensor back to PIL image\n",
        "          pil_image = to_pil_image(image)\n",
        "          pil_image.save(image_path)\n",
        "\n",
        "          # Save YOLOv5 format annotation\n",
        "          annotation_path = os.path.join(save_dir, f'{idx}.txt')\n",
        "          with open(annotation_path, 'w') as f:\n",
        "              for anno in annotations_for_image:\n",
        "                  f.write(f\"{anno[0]} {anno[1]} {anno[2]} {anno[3]} {anno[4]}\\n\")\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image = self.images[idx]\n",
        "      annotation = self.annotations[idx]\n",
        "      # Convert annotations to a tensor\n",
        "      annotation = torch.as_tensor(annotation, dtype=torch.float32)  # annotations are floats\n",
        "\n",
        "      return image, annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64u9Yr4MhnmT"
      },
      "source": [
        "The images in the dataset have different resolutions and sizes, although they are really close, every model needs similar resolutions in the same dataset so we are going to resize them to a standard resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm-JIVhtdjTL"
      },
      "outputs": [],
      "source": [
        "# My root folder, CHANGE PATH\n",
        "data_folder = folder + \"dataset/\"\n",
        "\n",
        "# This is the uniformed size I want my images to be (I'd rather upscale them to max)\n",
        "target_size = (376, 1242)\n",
        "\n",
        "# All the transformation I want to apply to images\n",
        "# The mean and std values are commonly used precalculated values for the ImageNet dataset.\n",
        "# normalized_value = (original_value - mean) / std\n",
        "transform = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      torchvision.transforms.Resize(target_size)\n",
        "])\n",
        "\n",
        "print(\"Transforming the dataset and creating the folders necessary for Yolov5, takes a few minutes.. \")\n",
        "dataset = CarDataset(data_folder, transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTkIPGEdfIjI"
      },
      "source": [
        "# Let's create our YAML file\n",
        "\n",
        "So Yolo needs a YAML file that contains some infos about our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvCljTXpFapV"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Extract dataset information\n",
        "num_classes = 1\n",
        "class_names = ['Car']\n",
        "# original images size after uniform, yolov5 usually works better with squared images\n",
        "#  but here the aspect ratio is too big so I wont square them\n",
        "image_size = [376, 1242]\n",
        "\n",
        "# Extract paths from the dataset object\n",
        "\n",
        "# Create YAML dictionary\n",
        "yaml_data = {\n",
        "    'train': data_folder + \"transformed/train\",\n",
        "    'val': data_folder + \"/transformed/val\",\n",
        "    'img_size': image_size,\n",
        "    \"nc\" : 1,    # number of classes\n",
        "    \"names\": [ 'car' ] # List of class names\n",
        "}\n",
        "\n",
        "# Save YAML file\n",
        "yaml_path = folder + 'cars.yaml'\n",
        "with open(yaml_path, 'w') as yaml_file:\n",
        "    yaml.dump(yaml_data, yaml_file, default_flow_style=False)\n",
        "\n",
        "print(f'YAML file saved at {yaml_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhr6OqZet4NL"
      },
      "source": [
        "# Yolov5\n",
        "Ive cloned yolov5 and put the folder in my drive to make the access easier ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzbKBuJfdowz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(folder + '/yolov5/')\n",
        "# Install requirements, all libraries used are here with their versions listed\n",
        "# PATH MUST BE FORCED HERE\n",
        "!pip install -U -r /content/drive/MyDrive/Bureau/VMI/yolov5/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Yolo's Plotting utilities for our metrics"
      ],
      "metadata": {
        "id": "Ihj7qYJueI_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/yolov5/utils')\n",
        "from plots import plot_results"
      ],
      "metadata": {
        "id": "J4F_5U50B_VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pt7TdjohWuo"
      },
      "source": [
        "**LETS TRAIN**\n",
        "\n",
        "* For the number of epochs, I started training for 10 then attempted 100 but it timedout for some Google-y reason so I ran it for 30 epochs and the results were good enough.\n",
        "* The most stable batch size was 8 this way the memory doesnt crash.\n",
        "* The parameters of the raining can be found in the cfg file.\n",
        "* Since we have a pretty limited dataset it is recommended that we used pre trained weights to initiate the training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blcb965vP-rr"
      },
      "outputs": [],
      "source": [
        "!python yolov5/train.py --epochs 20 --batch-size 8 --data /content/drive/MyDrive/Bureau/VMI/cars.yaml --cfg yolov5/models/yolov5s.yaml --weights /content/drive/MyDrive/Bureau/VMI/cars/yolov5/runs/train/exp/weights/best.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Yolo_Folder = folder + 'yolov5/'\n",
        "plot_results(Yolo_Folder + 'runs/train/exp/results.csv')\n",
        "# Display results of training\n",
        "fig, ax = plt.subplots(1)\n",
        "image = cv2.imread(Yolo_Folder + \"runs/train/exp/results.png\")\n",
        "ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4ZJUMRofHMcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLUPGP8fQAO8"
      },
      "outputs": [],
      "source": [
        "!python yolov5/val.py --data /content/drive/MyDrive/Bureau/VMI/cars/cars.yaml --weights /content/drive/MyDrive/Bureau/VMI/cars/yolov5/runs/train/exp/weights/best.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION PIPELINE\n",
        "\n",
        "The source here is the test images, and the weights is our trained model:"
      ],
      "metadata": {
        "id": "LTTI7m0sjwYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTxklLidQDSS"
      },
      "outputs": [],
      "source": [
        "!python yolov5/detect.py --source /content/drive/MyDrive/Bureau/VMI/cars/dataset/transformed/test --weights /content/drive/MyDrive/Bureau/VMI/cars/yolov5/runs/train/exp/weights/best.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi1W9IjrDvV_"
      },
      "source": [
        "# Displaying bounding boxes directly on our test imgaes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HhYLCETD140"
      },
      "outputs": [],
      "source": [
        "# Specify the image filename and annotation filename\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "# Define the mean and standard deviation used for reverse normalization\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "for filename in tqdm(os.listdir(\"/content/drive/MyDrive/Bureau/VMI/cars/yolov5/runs/detect/exp\")):\n",
        "            if filename.endswith(\".png\"):\n",
        "                image = cv2.imread(os.path.join(\"/content/drive/MyDrive/Bureau/VMI/cars/yolov5/runs/detect/exp\", filename)).astype(np.float32) / 255.0\n",
        "                # Define the mean and standard deviation used for normalization\n",
        "                mean = [0.485, 0.456, 0.406]\n",
        "                std = [0.229, 0.224, 0.225]\n",
        "\n",
        "                # Reverse the normalization using torchvision.transforms.Normalize\n",
        "                reverse_normalize = transforms.Compose([\n",
        "                    transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)], std=[1/s for s in std]),\n",
        "                ])\n",
        "\n",
        "                # Apply the reverse normalization\n",
        "                original_image = reverse_normalize(torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)).squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "                # Clip values to ensure they are in the valid range [0, 1]\n",
        "                original_image = np.clip(original_image, 0, 1)\n",
        "\n",
        "                # Convert back to uint8 (if necessary)\n",
        "                original_image = (original_image * 255).astype(np.uint8)\n",
        "\n",
        "                # Display the image with bounding box\n",
        "                # Create a new figure and axis for each image\n",
        "                fig, ax = plt.subplots()\n",
        "\n",
        "                # Since we read the image with cv2 (BGR) we have to display we plot and so in RGB\n",
        "                ax.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "                # Display the image and close the figure\n",
        "                plt.show()\n",
        "                plt.close(fig)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}